# TomeHub System - SRE Mental Stress Test Report
**Date:** February 2, 2026  
**System:** Async FastAPI + Oracle Pool + L1/L2 Cache + LLM Integration  
**Scenario:** Production high-load stress testing

---

## Executive Summary

| Metric | Current | Breaking Point | Risk Level |
|--------|---------|-----------------|------------|
| Pool Size | 20 conn | 15-18 â†’ Queueing | ğŸ”´ HIGH |
| Cache Hit Rate | 60-70% | <40% â†’ L2 thrash | ğŸŸ¡ MEDIUM |
| LLM Latency p95 | 2-3s | >5s â†’ Circuit open | ğŸ”´ HIGH |
| Streaming Timeout | 60s | 30s timeout â†’ Drops | ğŸŸ  MEDIUM |
| Memory per Worker | 200-300MB | >500MB â†’ OOM | ğŸ”´ HIGH |

---

## 1. LOAD SCENARIOS & FAILURE MODES

### Scenario 1A: Concurrent Search Queries (100 simultaneous)

**Load Profile:**
```
â”Œâ”€ 100 concurrent users
â”œâ”€ Each: /search (complex query, 5-7 second SLA)
â”œâ”€ Backend workers: 4 (Uvicorn)
â”œâ”€ Oracle pool: 20 connections
â””â”€ LLM calls per search: 2-3 (embeddings + generation)
```

**Timeline:**

```
T=0s:   100 requests arrive
        â”œâ”€ 4 workers start processing (4 workers Ã— 4 concurrent = 16 max concurrent)
        â”œâ”€ 84 requests queue in Uvicorn (FastAPI backlog)
        â””â”€ Database pool: 4 connections used, 16 available
        
T=0.5s: LLM embedding API calls start (100 Ã— 2 = 200 embedding requests)
        â”œâ”€ 20 circuit breaker slots filled
        â”œâ”€ 180 requests queue in circuit breaker
        â”œâ”€ p95 LLM latency: 2-3s
        â””â”€ Some requests start HALF_OPEN recovery test
        
T=1s:   Cache hit rate analysis
        â”œâ”€ L1 cache: 60 hits (60% of 100)
        â”œâ”€ L2 cache (Redis): 15 hits (15% of remaining 40)
        â”œâ”€ Database queries: 25 Ã— 3-5 queries each = 75-125 queries
        â”œâ”€ Database pool: 18-20 connections active
        â””â”€ Remaining 0-2 connections for new requests
        
T=3s:   LLM returns (first batch completed)
        â”œâ”€ Response generation starts
        â”œâ”€ Streaming begins (60+ concurrent streams)
        â”œâ”€ Memory: 4 workers Ã— 100MB per stream = 400MB
        â””â”€ Network: 60 streams Ã— 1-5 Mbps = 60-300 Mbps
        
T=3-7s: Timeout risk window
        â”œâ”€ Queued requests hit 5-7s SLA
        â”œâ”€ Users waiting >5s start retrying
        â”œâ”€ 20-30 retry requests add to queue
        â”œâ”€ Pool exhaustion: 20/20 connections + queue of 15-20
        â””â”€ New requests wait 2-3s just for connection
        
T=7s:   First requests complete, releases begin
        â”œâ”€ 40-50 requests complete successfully
        â”œâ”€ 30-40 requests timeout (SLA breach)
        â”œâ”€ 20-30 retried requests start processing
        â””â”€ Cascading effect: more retries than completions
        
T=10s:  System degraded
        â”œâ”€ Success rate: 50-60%
        â”œâ”€ Timeout rate: 20-30%
        â”œâ”€ Retry rate: 15-25%
        â””â”€ Response time p95: 8-12 seconds
        
T=15s:  Recovery phase (if no circuit breaker failure)
        â”œâ”€ Most original requests complete
        â”œâ”€ Retry queue drains
        â”œâ”€ Connection pool returns to normal
        â””â”€ System stabilizes
```

**Failure Mode: SLOW DEGRADATION**
- Not a hard crash, but graceful performance decline
- Success rate stays >50% but SLA consistently breached
- Timeouts cluster around the 5-7s window
- Retries extend problem for 5-10 more seconds

**Bottleneck Analysis:**
```
1. Database Connection Pool (20 conn)
   Issue: 100 concurrent search queries need ~100 DB calls
   Queue depth: 75-80 waiting for connection
   Wait time: 1-3s per connection
   
2. LLM Circuit Breaker (Single global instance)
   Issue: 200 embedding requests queued
   Max throughput: ~30 embeddings/sec (if no failures)
   Batch latency: 6-8 seconds for all 200
   Risk: If any fail, circuit opens for ALL searches
   
3. Async Task Queue
   Issue: FastAPI only has 4 workers
   Backpressure: 84/100 requests queued immediately
   Queueing latency: 1-2s before worker even touches request
   
4. Memory per concurrent stream
   Issue: 60 concurrent streams Ã— 1-5MB each = 60-300MB
   Worker process: ~200MB baseline + 60-300MB streaming
   Total per worker: 260-500MB Ã— 4 = 1-2GB
```

---

### Scenario 1B: Same Load, But Cache is Cold

**Assumptions:**
- Redis down or cache invalidated
- All 100 searches miss L1 + L2
- All 100 require fresh database queries + LLM calls

**Timeline:**

```
T=0s:   100 requests arrive
        â””â”€ All will miss cache (guaranteed)
        
T=0.5s: Database load
        â”œâ”€ 100 searches Ã— 5 queries = 500 DB queries
        â”œâ”€ Pool: 20 conn, 480 queued
        â”œâ”€ Queue depth: 480 waiting queries
        â””â”€ Average wait: 500 queries Ã· 20 conn Ã· T seconds
        
T=1s:   DB exhaustion phase
        â”œâ”€ Slow queries start (cache miss = full table scans)
        â”œâ”€ Query latency: 500ms â†’ 2s (without index)
        â”œâ”€ Timeout cascade: Queries waiting 2-3s
        â”œâ”€ Circuit breaker: 200 embedding requests piled up
        â””â”€ p95 latency: 8-12s already
        
T=2-3s: FAILURE POINT
        â”œâ”€ 30-40% of searches timeout (before completion)
        â”œâ”€ Retries add 50-100 more queries
        â”œâ”€ Database connection pool: 20/20 all waiting
        â”œâ”€ Memory: 4 workers Ã— 300MB = 1.2GB (streaming)
        â””â”€ System enters cascading failure
        
T=3-5s: Cascade worsens
        â”œâ”€ New requests start getting connection timeout (0ms response)
        â”œâ”€ Load balancer sees failures, might mark backend unhealthy
        â”œâ”€ Remaining cache hits can't keep up with failures
        â””â”€ Error rate: 40-60%
```

**Failure Mode: SUDDEN COLLAPSE**
- Not slow degradation, but sudden timeout wall
- Error rate jumps from 0% to 40%+ in <2 seconds
- Once database exhausted, recovery takes 30-60s
- If LLM circuit breaker also opens: complete failure for searches

**Cascade Chain:**
```
Cold cache â†’ 500 DB queries queued
          â†’ 20 connections all busy
          â†’ New requests timeout immediately
          â†’ Retries add to queue
          â†’ More timeouts
          â†’ Circuit breaker opens (if LLM also slow)
          â†’ Search completely fails for 5+ minutes
```

---

### Scenario 2: LLM Embedding API Degradation

**Load Profile:**
```
- 50 concurrent searches (moderate load)
- Each needs 2 embedding API calls
- LLM API becomes slow: 5s latency (vs normal 1-2s)
- Then fails: 50% error rate for 10 minutes
```

**Timeline (Without Circuit Breaker):**
```
T=0s:   50 searches start
        â”œâ”€ 100 embedding requests sent to LLM API
        â””â”€ Normal latency: 1-2s
        
T=2s:   LLM performance degrades (5s latency observed)
        â”œâ”€ All 50 requests now blocked waiting for embeddings
        â”œâ”€ Database work done, waiting for LLM
        â”œâ”€ Response queue building
        â””â”€ Worker threads: All 4 waiting on LLM I/O
        
T=5s:   First embedding responses return
        â”œâ”€ But new requests continue arriving
        â”œâ”€ New embeddings sent to slow LLM
        â”œâ”€ Queue depth: 30-40 awaiting LLM
        â””â”€ Response time: 5s LLM + 2s DB + 1s generation = 8s
        
T=10s:  LLM API fails outright (50% errors)
        â”œâ”€ 50 new requests, 25 embedding calls fail
        â”œâ”€ Without circuit breaker: retries compound
        â”œâ”€ Retry storm: 25 failed Ã— 3 retries = 75 extra requests
        â”œâ”€ Total: 75 requests queued for failed LLM
        â””â”€ New users continue arriving â†’ queue grows to 200+
        
T=15s:  Cascading failure
        â”œâ”€ All searches blocked on LLM
        â”œâ”€ Error rate: 75%+ (everything times out)
        â”œâ”€ Queue depth: 200+ requests
        â”œâ”€ Recovery time: When LLM API recovers (10-30 min? manually?)
        â””â”€ System completely broken for searches
```

**Failure Mode: RESOURCE STARVATION**
- All 4 workers blocked on slow LLM I/O
- No capacity for new requests
- Queue grows unbounded
- Memory exhaustion: Each waiting request = 1-5MB
- With 200+ queued: 200-1000MB additional memory

**Impact Without Phase 2 (Circuit Breaker):**
```
âŒ No fast-fail mechanism
âŒ Requests wait full timeout (20-30s in some cases)
âŒ Retry amplification: 1 failure â†’ 3 retries â†’ 3 more failures
âŒ Memory leak: Waiting requests accumulate in queue
âŒ Eventually: OOM crash or kernel kills process
```

**Impact WITH Phase 2 (Circuit Breaker):**
```
âœ… Circuit opens after 5 failures
âœ… Fast-fail: 1ms rejection instead of 20s timeout
âœ… No retry amplification
âœ… Graceful degradation: Keyword search still works
âœ… Recovery: Automatic retry after 5 minutes
```

---

### Scenario 2B: Long-Running Ingestion + Peak Search Load

**Load Profile:**
```
- Batch ingestion: 10,000 documents being processed
- Each document: 5-10 DB inserts + 1 embedding call
- Concurrently: 50 user searches arrive
```

**Timeline:**

```
T=0s:   Ingestion starts
        â”œâ”€ 10,000 docs Ã— 5 inserts = 50,000 DB writes
        â”œâ”€ Database pool: 10 connections reserved for ingestion
        â”œâ”€ Available for searches: 10 connections
        â””â”€ LLM capacity: 50% reserved for ingestion embeddings
        
T=30s:  Peak search load arrives (50 concurrent users)
        â”œâ”€ Each search needs: 5 DB queries + 2 embeddings
        â”œâ”€ Available pool: Only 10 connections
        â”œâ”€ Queue depth: 50 searches Ã— 5 queries = 250 queued
        â”œâ”€ LLM: Already handling 10,000 embedding requests
        â”œâ”€ Circuit breaker: 50% capacity left (if any)
        â””â”€ Database queue: 250+ waiting
        
T=60s:  Database becomes bottleneck
        â”œâ”€ Ingestion: 50,000 writes in progress
        â”œâ”€ Searches: 250 queries queued
        â”œâ”€ Pool: 20/20 all ingestion-related
        â”œâ”€ Search latency: p95 = 30s+ (beyond SLA)
        â”œâ”€ Search error rate: 40-50% (timeout)
        â””â”€ Users experience "search is broken"
        
T=120s: Ingestion still ongoing
        â”œâ”€ 60% of original documents processed
        â”œâ”€ User searches: Still queued, still timing out
        â”œâ”€ Cascading retries: 50-100 more requests
        â”œâ”€ Error logs: Flooded with timeouts
        â””â”€ System appears broken (but it's just slow)
        
T=180s: Ingestion complete
        â”œâ”€ All 10,000 documents inserted
        â”œâ”€ 10 connections freed up
        â”œâ”€ Search queue finally drains
        â”œâ”€ Searches complete: 8-15s latency (delayed but successful)
        â””â”€ Users see recovered system
```

**Failure Mode: RESOURCE CONTENTION**
- Not a crash, but catastrophic slow-down
- Both operations blocked each other
- Search SLA breached for 3-5 minutes
- User experience: "System is down" even though it's working

**Root Cause:**
```
Problem 1: Single shared database pool
- No QoS or priority queuing
- Ingestion eats all 20 connections
- Searches starved

Problem 2: Single shared LLM circuit breaker
- Ingestion embeddings use 80% capacity
- Only 20% left for search embeddings
- Circuit breaker can't differentiate priority

Problem 3: No load shedding
- Both ingestion + search try to complete
- Neither gets enough resources
- Both slow to crawl

Solution needed:
- Separate pools for read vs write
- Priority queuing (search >urgent > ingestion)
- Load shedding (reject low-priority under high load)
```

---

### Scenario 3: Memory Exhaustion with Streaming

**Load Profile:**
```
- 40 concurrent searches with streaming responses
- Each stream: 1-5MB response (depends on richness)
- Worker process baseline: 200MB
- Total worker capacity: ~500MB (per worker)
```

**Timeline:**

```
T=0s:   40 concurrent streams start
        â”œâ”€ Worker 1: 200MB baseline + 40 Ã— 1MB = 240MB
        â”œâ”€ Worker 2: 200MB baseline + 40 Ã— 1MB = 240MB
        â”œâ”€ Worker 3: 200MB baseline + 40 Ã— 1MB = 240MB
        â”œâ”€ Worker 4: 200MB baseline + 40 Ã— 1MB = 240MB
        â””â”€ Total: ~960MB (under 1GB, OK)
        
T=1s:   Streams build up
        â”œâ”€ Some streams complete (100-200ms)
        â”œâ”€ New requests queue behind existing streams
        â”œâ”€ Average stream duration: 2-3s (slow network)
        â”œâ”€ Concurrent streams: 40 still active + 20 new arriving
        â”œâ”€ Per worker now: 200MB + 60 Ã— 1MB = 260MB
        â””â”€ Total: ~1.04GB (approaching memory pressure)
        
T=2s:   Network becomes slow
        â”œâ”€ Client bandwidth limited (slow 3G/4G)
        â”œâ”€ Streams take 5-10s to complete instead of 2s
        â”œâ”€ Concurrent count: 40 + 30 new = 70 total
        â”œâ”€ Per worker: 200MB + 70 Ã— 1MB = 270MB
        â”œâ”€ Plus buffer for response bodies
        â””â”€ Total: ~1.3GB (memory pressure!)
        
T=3-4s: MEMORY PRESSURE PHASE
        â”œâ”€ Linux kernel: Memory pressure > 80%
        â”œâ”€ Swap I/O triggered (if swap exists)
        â”œâ”€ Response latency jumps: 5s â†’ 15s (due to swap)
        â”œâ”€ New stream setup slower (memory allocation struggle)
        â”œâ”€ Garbage collection running more frequently
        â””â”€ Worker threads: Slowing due to GC pauses
        
T=5s:   OOM killer activates
        â”œâ”€ If memory pressure > 95%
        â”œâ”€ Kernel randomly kills process
        â”œâ”€ Scenario A: Kills worker (70 streams dropped)
        â”œâ”€ Scenario B: Kills entire Python process (all 4 workers down)
        â””â”€ Result: 280-1120 users see connection reset
        
```

**Failure Mode: SILENT RESOURCE EXHAUSTION**
- System doesn't report being out of memory
- Streams just slowly get dropped
- Users see: "Connection reset by peer"
- Logs: Maybe OOMkiller mention, but buried
- SRE sees: Intermittent connection drops, no clear cause

**Memory Amplification:**
```
Response body alone: 1-5MB per stream
Per-stream overhead:
  - Python object overhead: 50-100KB
  - Async task state: 50KB
  - Buffer cache: 100-500KB (if buffering response)
  - Socket buffer: 64KB (TCP send buffer)
  
Real memory per stream: 200KB - 1MB beyond response body

With 70 concurrent streams:
  - 70 Ã— 3MB avg (body + overhead) = 210MB
  - 4 workers Ã— 200MB baseline = 800MB
  - Total: 1GB just for this scenario
  
Plus system overhead:
  - OS page cache: 100-200MB
  - Other processes: 100-500MB
  
Total system memory with 40-70 streams: 1.2-1.5GB
```

**Mitigation Gaps:**
```
âŒ No stream buffer limit per request
âŒ No max concurrent streams hard limit (e.g., reject if >100)
âŒ No memory monitoring/alerting
âŒ No graceful degradation on memory pressure
âŒ No backpressure mechanism (server stops accepting streams)
```

---

### Scenario 4: Database Query Amplification (N+1 Problem)

**Scenario:** Search returns 100 results, frontend requests details for each

**Timeline:**

```
T=0s:   User searches: "What is Dasein?"
        â””â”€ Single query: SELECT * FROM TOMEHUB_CONTENT WHERE ... LIMIT 100
        
T=1s:   Results arrive (100 items)
        â””â”€ Sends back 100 IDs: [1, 2, 3, ..., 100]
        
T=1.5s: Frontend makes 100 individual detail requests
        â”œâ”€ GET /api/content/1
        â”œâ”€ GET /api/content/2
        â”œâ”€ ...
        â”œâ”€ GET /api/content/100
        â””â”€ Results in 100 parallel DB queries!
        
T=2s:   Database load spike
        â”œâ”€ Expected: 1 query (already done)
        â”œâ”€ Actual: 100 queries hitting database
        â”œâ”€ Pool: 20 connections, 80 queries queued
        â”œâ”€ Each query: 50-200ms from cache miss
        â”œâ”€ Total time: 5-10 seconds
        â””â”€ User sees: 5-10s delay for detail view
        
T=12s:  With 10 concurrent users doing same thing
        â”œâ”€ 10 users Ã— 100 detail requests = 1,000 queries
        â”œâ”€ Pool: 20 connections, 980 queued
        â”œâ”€ Wait time per query: 1,000 Ã· 20 Ã· 10sec = 5 seconds
        â”œâ”€ User experience: 15+ seconds to load details
        â””â”€ Appears as system slowdown
```

**Failure Mode: QUERY AMPLIFICATION**
- Single user request triggers hidden N+1 queries
- With 10 concurrent users: 1,000 queries instead of expected 10-20
- Database thrashing from poorly designed API contract
- No error, just slow degradation

**Impact:**
```
If undetected:
- Scales badly: 100 users = 10,000 queries (system collapse)
- Memory: Each queued query holds state (connection, buffers)
- CPU: Query parsing, optimization, execution overhead

Detection clues:
- Database CPU 100% despite low user count
- Query logs: Thousands of identical detail queries
- API latency: Slow response even with cache hits elsewhere
```

---

## 2. FAILURE MODE MATRIX

### What Fails and Why?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Failure Mode               â”‚ Symptom           â”‚ Root Cause      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SLOW DEGRADATION          â”‚ p95 latency â†‘     â”‚ Queue buildup   â”‚
â”‚                           â”‚ SLA breach        â”‚ Resource pool   â”‚
â”‚                           â”‚ But success âœ“     â”‚ exhaustion      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SUDDEN COLLAPSE           â”‚ Error rate â†‘ fast â”‚ Hard limit hit  â”‚
â”‚                           â”‚ Timeout wall      â”‚ (connection,    â”‚
â”‚                           â”‚ No recovery       â”‚ memory, thread) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RESOURCE STARVATION       â”‚ Requests queued   â”‚ Worker threads  â”‚
â”‚                           â”‚ blocked waiting   â”‚ all blocked on  â”‚
â”‚                           â”‚ Memory leak       â”‚ I/O (LLM, DB)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CASCADING FAILURE         â”‚ Retries amplify   â”‚ No circuit      â”‚
â”‚                           â”‚ More failures     â”‚ breaker, retry  â”‚
â”‚                           â”‚ Load increases    â”‚ exponential      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SILENT EXHAUSTION         â”‚ Intermittent      â”‚ Memory/connectionâ”‚
â”‚                           â”‚ connection reset  â”‚ pool leaks      â”‚
â”‚                           â”‚ No error message  â”‚ Undetected      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CACHE THRASHING           â”‚ Hit rate â†“        â”‚ Cache size too  â”‚
â”‚                           â”‚ Latency â†‘         â”‚ small, eviction â”‚
â”‚                           â”‚ More DB queries   â”‚ rate too high   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Time to Failure

```
Scenario                    â”‚ TFF     â”‚ Recovery Time â”‚ Impact
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
100 concurrent searches     â”‚ 5-7s    â”‚ 10-15s        â”‚ SLA breach
Cold cache (100 users)      â”‚ 1-2s    â”‚ 30-60s        â”‚ Collapse
LLM API fails               â”‚ 5s      â”‚ 10-30 min     â”‚ Complete outage
Memory pressure (70 streams)â”‚ 3-5s    â”‚ Process dies  â”‚ Hard restart
Database pool exhaustion    â”‚ 2-3s    â”‚ 15-30s        â”‚ All searches fail
```

---

## 3. BOTTLENECK ANALYSIS

### Critical Bottleneck #1: Database Connection Pool (20 connections)

**Current Spec:**
```
Pool Size: 20 connections
Max Concurrency: 20 simultaneous queries
Queue: Unbounded (grows until OOM or timeout)
```

**Breaking Point:**
```
Capacity: 20 concurrent queries
100 concurrent users (each needs 5 queries):
  - Required: 100 Ã— 5 = 500 connections
  - Available: 20
  - Queue depth: 480
  - Queue latency: 480 Ã· 20 Ã· 10sec = 2.4 seconds average
  
Add 3-5 second query latency:
  - Total per-request latency: 5-7 seconds
  - SLA: 5-7 seconds
  - Result: 50% of requests breach SLA
```

**Failure Progression:**
```
Step 1: Queue builds (0-100ms)
  - All 20 connections in use
  - 80 requests waiting
  
Step 2: Queue timeout (1-2s)
  - Requests waiting timeout
  - Retries add to queue
  
Step 3: Cascade (2-3s)
  - More timeouts than completions
  - Queue grows exponentially
  
Step 4: Collapse (3-5s)
  - New requests immediately timeout
  - System appears broken
```

**Mitigation Options:**
```
Option A: Increase pool size to 50
  - Pro: Handles 50 concurrent users better
  - Con: Database license costs, connection limits
  
Option B: Connection pooling/multiplexing
  - Pro: Many logical connections, fewer physical
  - Con: Complexity, Oracle dialect support
  
Option C: Query optimization
  - Pro: Reduce queries per request (5 â†’ 1-2)
  - Con: Schema redesign, query rewrites
  
Option D: Read replicas
  - Pro: Distribute read-heavy queries
  - Con: Cost, replication lag
```

---

### Critical Bottleneck #2: LLM Circuit Breaker (Single Global Instance)

**Current Spec:**
```
Circuit Breaker: 1 shared instance for all embeddings
Failure threshold: 5 consecutive failures
State transitions: CLOSED â†’ OPEN â†’ HALF_OPEN
Recovery timeout: 5 minutes
```

**Breaking Point:**
```
Scenario: 200 embedding requests (100 searches Ã— 2 embeddings each)

Without failure:
  - LLM throughput: 30-50 embeddings/sec
  - Total time: 200 Ã· 40 = 5 seconds
  - OK: Searches complete in 7-8 seconds
  
With degraded LLM (5s latency):
  - LLM throughput: 1 embedding per 5 seconds = 0.2/sec
  - Total time: 200 Ã· 0.2 = 1,000 seconds!
  - Result: Searches timeout before first response
  
With LLM API failures (50% error rate):
  - 200 requests â†’ 100 fail
  - Circuit breaker: Counts failures across ALL searches
  - After 5 failures: OPENS
  - Result: ALL searches fail (not just those hitting failing API)
  - Recovery: 5 minutes of complete failure
```

**Single Global Point of Failure:**
```
Issue: One circuit breaker for all embedding requests
Problem: Failure in embeddings affects BOTH:
  - Semantic search (needs embeddings)
  - Query expansion (uses embeddings)
  - AI generation (might use embeddings)

If circuit opens:
  - No semantic search possible
  - Fallback to keyword search only
  - AI generation quality degrades
  - Search success rate: 40-60% (depends on fallback)

Recovery dependency:
  - Blocked on LLM API health
  - Can't proceed for 5 minutes (hardcoded timeout)
  - Manual intervention: Delete `.deployed` or restart
```

**Mitigation Options:**
```
Option A: Multiple circuit breakers
  - One per task type (query expansion, embeddings for search, etc.)
  - Pro: Failure isolation
  - Con: Complexity, more state to track
  
Option B: Adaptive circuit breaker
  - Instead of binary OPEN/CLOSED, reduce traffic % by % when degraded
  - Pro: Graceful degradation instead of hard failure
  - Con: More complex logic, harder to reason about
  
Option C: Bulkheads
  - Separate thread pools for each circuit breaker
  - Pro: One failure doesn't starve other operations
  - Con: Thread pool overhead, Python GIL contention
```

---

### Critical Bottleneck #3: Memory Per Concurrent Stream

**Current Spec:**
```
Worker baseline: 200MB
Per-stream overhead: 200KB - 1MB (including response buffer)
Max memory per worker: ~500MB
Workers: 4
Total system memory: 2GB (typical VM)
```

**Breaking Point:**
```
Per worker max concurrent streams:
  - Available: 500MB - 200MB = 300MB
  - Per stream: 1MB (conservative)
  - Max: 300 streams per worker
  - Actual achievable: 100-150 (due to GC overhead, Python internals)

Across 4 workers:
  - Max: 400-600 concurrent streams
  - Reality: System degradation starts at 200-300

With slow network (10Mbps):
  - Each stream takes 5-10 seconds
  - If 50 requests/sec arrive: Queue grows
  - Concurrent count: 50 req/sec Ã— 5 sec = 250 concurrent
  - Memory: 250 Ã— 1MB = 250MB (plus 800MB baseline) = 1.05GB
  - Swap kicks in â†’ Performance degrades 50%+

With OOM:
  - Kernel kills worker process
  - All ~250 streams dropped
  - Cascading failures for other requests
```

**Failure Timeline:**
```
T=0:    System normal, 50 MB free memory
T+1s:   200 requests arrive, 200 streams start
        Memory: 800MB (baseline) + 200MB (streams) = 1GB
        Free: 1GB remaining
T+2s:   Network slow, streams not completing
        Concurrent: Still 150-180 active
        Memory: Stable at 1.05GB
T+3s:   GC pressure increases
        Garbage collection pause: 100-500ms
        User experience: Slow stream responses
T+4s:   Memory pressure > 80%
        Swap usage: 100-200MB
        Performance: Degraded 30-50%
T+5s:   New streams slower to start (memory allocation delay)
        Network still slow, concurrent streams accumulate
        Memory: 1.2GB+ (including swap)
T+6s:   OOMkiller triggered
        Kills worker or entire process
        Result: Abrupt connection reset for all streams
```

**Silent Exhaustion Risk:**
- No error message saying "out of memory"
- Just "connection reset" from client perspective
- Logs might show OOMkiller, but buried in syslog
- Appears intermittent, hard to reproduce

---

### Critical Bottleneck #4: Async Task Queue (Uvicorn Workers)

**Current Spec:**
```
Uvicorn workers: 4
Backlog per worker: ~1000 (OS limit, tunable)
Total backlog: ~4000 requests
```

**Breaking Point:**
```
Request arrival rate: 100 requests/sec
Worker processing rate: 10 requests/sec (slow searches)
Queue buildup: 100 - 10 = 90 requests/sec

Backlog growth:
T=0s:   Backlog: 0
T+1s:   Backlog: 90
T+2s:   Backlog: 180
T+3s:   Backlog: 270
T+4s:   Backlog: 360
T+5s:   Backlog: 450 (getting concerning)
T+10s:  Backlog: 900 (80% of capacity)
T+11s:  Backlog: 990 (exceeds OS default)
T+12s:  New requests: REJECTED (connection refused)
```

**Consequences:**
```
When backlog exceeds capacity:
  - Load balancer gets SYN_RECEIVED (TCP accept queue full)
  - Client sees "Connection refused"
  - Not a timeout, but immediate rejection
  - User experience: "Server down" (not "server slow")
  
When requests finally process:
  - Average wait time: 2-5 seconds
  - Response time: wait + processing
  - Total: 8-15 seconds (vs SLA 5-7)
  - User sees: System is slow
  
If all 4 workers blocked:
  - New requests queued in OS kernel
  - OS queues: 1000-2000 requests (varies by tuning)
  - Total wait: 100-200 seconds for new requests
  - Result: Complete system unavailability
```

---

## 4. WRONG ANSWER SCENARIOS

### When TomeHub Returns Incorrect Results

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario                   â”‚ Cause          â”‚ Detectability  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stale cached answer        â”‚ Cache version  â”‚ Hard (if users â”‚
â”‚ with old prompt result     â”‚ not bumped     â”‚ don't notice) â”‚
â”‚                            â”‚ (Phase 3 fix)  â”‚ Low without    â”‚
â”‚                            â”‚                â”‚ Phase 3        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Embedding version changed, â”‚ Model updated  â”‚ Hard (silently â”‚
â”‚ cache key not updated      â”‚ but search key â”‚ returns stale  â”‚
â”‚                            â”‚ unchanged      â”‚ vectors)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Partial/corrupted response â”‚ Stream cut off â”‚ Easy (JSON     â”‚
â”‚ from network timeout       â”‚ mid-response   â”‚ parse error)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM circuit open, fallback â”‚ Keyword search â”‚ Detectable     â”‚
â”‚ returns suboptimal results â”‚ only, lower    â”‚ (different     â”‚
â”‚ (keyword-only search)      â”‚ quality        â”‚ result ranking)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database query timeout,    â”‚ Partial result â”‚ Noticeable     â”‚
â”‚ missing 30% of chunks      â”‚ set returned   â”‚ (fewer results)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dual-AI Judge fails, Work  â”‚ No evaluation  â”‚ Hard (low      â”‚
â”‚ AI returns unreviewed      â”‚ of answer      â”‚ quality slips  â”‚
â”‚ low-quality answer         â”‚ quality        â”‚ through)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example: Prompt Change Without Version Bump

```
Timeline:

Day 1: Deploy with LLM_MODEL_VERSION=v1
  - Prompt: "Answer as a philosopher"
  - Answer for "What is Dasein?": Ontological perspective
  - Cached with key: ["what is dasein", v1]

Day 2: Change prompt without bumping version
  - Prompt: "Answer as a neuroscientist"
  - But still LLM_MODEL_VERSION=v1 (forgot to bump!)

Day 3: User searches "What is Dasein?"
  - Cache lookup: ["what is dasein", v1]
  - Result: Old cached answer (philosophical)
  - Expected: New answer (neuroscientific)
  - User sees: Wrong perspective, without knowing

  WITH PHASE 3 FIX:
  - Developer must change LLM_MODEL_VERSION=v1 â†’ v2
  - Cache key changes: ["what is dasein", v1] â†’ ["what is dasein", v2]
  - No cache hit
  - New answer generated with new prompt
  - User gets correct (new) answer
```

---

## 5. TIMEOUT SCENARIOS

### What Times Out Under Load?

```
Component               â”‚ Timeout     â”‚ Under Load    â”‚ Impact
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Search endpoint         â”‚ 30s (HTTP)  â”‚ At 5-7s load  â”‚ Early timeout
LLM embedding call      â”‚ 60s         â”‚ At 2-3s       â”‚ Circuit breaks
Database query          â”‚ 30s (Oracle)â”‚ At 1-2s       â”‚ Connection lost
Stream response         â”‚ 60s         â”‚ At 5-10s      â”‚ Connection reset
Cache.get()             â”‚ 5s (Redis)  â”‚ Never        â”‚ Fallback to DB
User sees error         â”‚ SLA: 5-7s   â”‚ Breached      â”‚ Error page shown
```

### Timeout Cascade Under Database Exhaustion

```
T=0s:   Database pool: 20/20 occupied
        New query arrives
        
T=0.1s: Query waits for connection (none available)
        Queue: 1 item
        
T+1s:   No connections freed
        Queue: 100+ items
        New query added to queue
        
T+5s:   Original query finally gets connection
        Executes (200ms)
        
T+5.2s: Query returns, but...
        By now, client timeout expired!
        Client sees: Connection timeout
        Server still processing result
        
T+5.3s: Server finishes, sends result to dead client
        Network sends anyway (wastes bandwidth)
        
Result: Work done, result discarded, users see timeout
```

---

## 6. MITIGATION STRATEGIES

### Short-term (Immediate, <1 day)

```
1. Increase connection pool: 20 â†’ 40
   Cost: Oracle license increase
   Benefit: Handles 100 concurrent searches
   
2. Reduce worker timeouts: 30s â†’ 10s
   Cost: Faster failure detection
   Benefit: Better error messaging, faster retry
   
3. Add request rate limiting: 1000 req/sec â†’ 500 req/sec
   Cost: Some requests rejected
   Benefit: Prevents queue explosion, protects system

4. Monitor Phase 2 (Circuit Breaker) health
   - Check /api/health/circuit-breaker
   - Alert on state change to OPEN
   - Manual recovery if needed (restart app)

5. Monitor Phase 3 (Version Validation)
   - Verify .deployed file exists and is current
   - Alert on version validation failure at startup
```

### Medium-term (1-2 weeks)

```
1. Optimize queries: 5 queries/search â†’ 2 queries/search
   Reduce database load by 60%
   
2. Implement query result caching: 24-hour TTL
   Reduce DB queries further
   
3. Use read replicas for expensive queries
   Distribute load across multiple DB instances
   
4. Implement bulkheads: Separate pools for ingestion vs search
   Prevent one operation starving the other
   
5. Add memory monitoring and alerting
   Alert when memory usage > 70%
   Auto-restart workers if > 85%
```

### Long-term (1-3 months)

```
1. Database query redesign
   - Denormalize some tables
   - Add materialized views for common queries
   - Reduce N+1 queries
   
2. Multiple circuit breakers
   - Separate by operation type
   - Prevent cascading failure
   
3. Implement graceful degradation tiers
   Tier 1: Search with embeddings (full quality)
   Tier 2: Search without embeddings (keyword only)
   Tier 3: Return cached results only
   
4. Dedicated ingestion worker pool
   - Separate from user-facing requests
   - Can be slower without impacting search
   
5. Elasticsearch or similar
   - Offload full-text search from Oracle
   - Much faster for large result sets
```

---

## 7. CURRENT STATE ASSESSMENT

### What Phase 1 (Firebase Auth) Improved

âœ… **Security:** Prevents unauthorized API access  
âœ… **Accounting:** Tracks which user made which request  
âœ… **Rate limiting:** Per-user limits prevent abuse  
âœ… **Audit trail:** All requests logged with user context  

âš ï¸ **Performance:** No impact (middleware < 1ms overhead)  
âœ… **Reliability:** Auth failures are fast (1-2ms), not slow

---

### What Phase 2 (Circuit Breaker) Improved

âœ… **LLM API resilience:** Prevents cascading failures  
âœ… **Fast failure:** 1ms rejection instead of 20s timeout  
âœ… **Auto-recovery:** 5-minute timeout, automatic retry  
âœ… **Graceful degradation:** Search continues with keywords  

âš ï¸ **Single point of failure:** One circuit breaker for all embeddings  
âš ï¸ **Manual recovery:** Restart app if circuit stuck (rare)  

**Impact on Stress Test:**
```
Scenario 1A (100 concurrent searches):
  Without Phase 2: Cascading LLM failures, error rate 30-40%
  With Phase 2: Clean failure at 5 minutes, error rate 5-10%
  
Scenario 2 (LLM degradation):
  Without Phase 2: System appears broken for 10-30 minutes
  With Phase 2: System fails fast, recovers in 5 minutes
```

---

### What Phase 3 (Version Validation) Improved

âœ… **Cache safety:** Never reuses stale cached results  
âœ… **Deployment safety:** Prevents version mismatch bugs  
âœ… **Clear errors:** Startup validation catches mistakes  
âœ… **Automatic enforcement:** No manual version tracking  

âš ï¸ **Doesn't prevent:** Cold cache loads, slow database queries  
âš ï¸ **Doesn't help:** High-load scenarios (just makes them safer)  

**Impact on Stress Test:**
```
Scenario: "What is Dasein?" search after prompt change
  Without Phase 3: Returns old cached answer (wrong!)
  With Phase 3: Would have prevented code from deploying
                or caught mismatch before cache hit
  
Scenario: Cold cache + load test
  Without Phase 3: Wrong answers possible (unknowable)
  With Phase 3: Answers guaranteed consistent (knowable wrong/right)
```

---

## 8. REMAINING VULNERABILITIES

### Not Covered by Phases 1-3

```
Vulnerability              â”‚ Severity â”‚ Mitigation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
High concurrency timeout   â”‚ ğŸ”´ HIGH  â”‚ Connection pool â†‘
                           â”‚          â”‚ Query optimization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cold cache (Redis down)    â”‚ ğŸ”´ HIGH  â”‚ L1 cache tuning
                           â”‚          â”‚ Query caching
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Memory exhaustion          â”‚ ğŸŸ¡ MEDIUMâ”‚ Memory limits
                           â”‚          â”‚ Stream backpressure
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Database replication lag   â”‚ ğŸŸ¡ MEDIUMâ”‚ Read-after-write
                           â”‚          â”‚ Consistency checks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
N+1 query pattern          â”‚ ğŸŸ¡ MEDIUMâ”‚ API redesign
                           â”‚          â”‚ Batch queries
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM rate limiting          â”‚ ğŸŸ  MEDIUMâ”‚ Token bucket
                           â”‚          â”‚ Queueing strategy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ingestion blocking search  â”‚ ğŸŸ  MEDIUMâ”‚ Separate pools
                           â”‚          â”‚ Priority queueing
```

---

## 9. FAILURE DECISION TREE

### How to Diagnose System State Under Load

```
Symptom: "Search is slow"
â”œâ”€ Time: Always slow? Or sudden?
â”‚  â”œâ”€ Gradual: Pool exhaustion (increase pool size)
â”‚  â””â”€ Sudden: Cache miss or DB unavailable
â”œâ”€ Error rate: High? Or just latency?
â”‚  â”œâ”€ High errors (>10%): Database or LLM failure
â”‚  â””â”€ Low errors: Just slow, queue buildup
â””â”€ Check: Logs for timeouts, queue depth, connection count

Symptom: "Intermittent connection reset"
â”œâ”€ Memory usage: Check ps aux | grep python
â”‚  â”œâ”€ >1.5GB: OOMkiller likely
â”‚  â””â”€ <1GB: Network issue
â”œâ”€ Stream count: lsof | grep socket
â”‚  â”œâ”€ >500: Too many concurrent streams
â”‚  â””â”€ <100: Network might be slow

Symptom: "Wrong answer returned"
â”œâ”€ Is cache involved? Check request for 'cache_hit'
â”‚  â”œâ”€ Yes: Version mismatch? Check .deployed
â”‚  â””â”€ No: LLM or judge AI issue
â”œâ”€ Recent code changes? Check git log --oneline -n 20
â”‚  â”œâ”€ Yes: Did you bump versions? (Phase 3 check)
â”‚  â””â”€ No: Database issue likely

Symptom: "LLM seems to be down"
â”œâ”€ Check: GET /api/health/circuit-breaker
â”‚  â”œâ”€ state: "open": Circuit open, wait 5 min or restart
â”‚  â”œâ”€ state: "half_open": Recovery in progress, wait 30s
â”‚  â”œâ”€ state: "closed": Circuit fine, issue elsewhere
â””â”€ If open: Check LLM API health independently
```

---

## 10. FINAL STRESS TEST SUMMARY

### Breaking Points by Load Level

```
Load         â”‚ Latency p95 â”‚ Error Rate â”‚ Failure Mode
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
10 users     â”‚ 2-3s        â”‚ 0%         â”‚ None
50 users     â”‚ 3-5s        â”‚ 0%         â”‚ None
100 users    â”‚ 5-8s        â”‚ 5-10%      â”‚ Slow degradation
150 users    â”‚ 8-15s       â”‚ 15-25%     â”‚ Gradual collapse
200+ users   â”‚ 20-30s+     â”‚ 40%+       â”‚ Complete failure
```

### What Fails First Under Load

```
1. Database connection pool (20 â†’ 100 needed) [CRITICAL]
   - Timeout: 2-3 seconds after SLA breach
   - Impact: 50% of searches fail

2. LLM circuit breaker (50% error rate on API)
   - Timeout: 5 seconds (queue buildup)
   - Impact: Fallback to keyword search, 50% quality loss

3. Memory on streaming (40+ concurrent streams)
   - Timeout: OOMkiller after 3-5s sustained load
   - Impact: Abrupt connection resets

4. Task queue backlog (Uvicorn)
   - Timeout: New requests rejected after 10 seconds load
   - Impact: "Connection refused" for new users

5. Cache (if Redis down)
   - Timeout: All searches slow (database only)
   - Impact: 5-10x latency increase
```

### Mitigation Priority

```
Priority 1 (Before 100 users):
  âœ… Implement Phase 2 (Circuit Breaker) - DONE
  âœ… Implement Phase 3 (Version Validation) - DONE
  âœ… Implement Phase 1 (Firebase Auth) - DONE
  â³ Increase DB pool: 20 â†’ 40 connections
  â³ Add memory monitoring and alerting

Priority 2 (Before 500 users):
  â³ Query optimization (5 â†’ 2 queries/search)
  â³ Implement read replicas
  â³ Caching layer for frequent queries

Priority 3 (Before 2000+ users):
  â³ Elasticsearch for full-text search
  â³ Multiple circuit breakers
  â³ Dedicated ingestion worker pool
  â³ Database sharding
```

---

## CONCLUSION

**Current State (With Phases 1-3):**
- âœ… Secure (Firebase Auth prevents unauthorized access)
- âœ… Resilient to LLM failures (Circuit breaker + retry logic)
- âœ… Safe from cache bugs (Version validation enforces bumps)
- âš ï¸ Can handle ~50-100 concurrent users reliably
- âŒ Breaks under 200+ concurrent users (database bottleneck)

**Biggest Risks (Unfixed):**
1. Database pool exhaustion (CRITICAL)
2. Memory exhaustion under high streaming load (HIGH)
3. Cold cache performance (MEDIUM)
4. Ingestion blocking search operations (MEDIUM)

**Recommended Next Actions:**
1. Load test with 100-200 concurrent users
2. Profile to identify actual bottleneck (likely database)
3. Increase connection pool and measure improvement
4. Implement query optimization to reduce DB load
5. Deploy memory monitoring and alerting
6. Add circuit breaker health checks to monitoring dashboard

---

**Report Generated:** February 2, 2026  
**System:** TomeHub with Phase 1, Phase 2, Phase 3 complete  
**Status:** Production-ready for low-to-moderate load (50-100 concurrent users)  
**Next:** Load testing and database optimization
